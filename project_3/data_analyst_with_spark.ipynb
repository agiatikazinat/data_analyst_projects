{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate() \n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark DataFrames basic example\").config(\"spark.some.config.option\", 'some-value').getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp_No: integer (nullable = true)\n",
      " |-- Emp_Name: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.csv(\"employees.csv\", header = True, inferSchema = True)\n",
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+----------+\n",
      "|Emp_No|Emp_Name|Salary|Age|Department|\n",
      "+------+--------+------+---+----------+\n",
      "|   198|  Donald|  2600| 29|        IT|\n",
      "|   199| Douglas|  2600| 34|     Sales|\n",
      "|   200|Jennifer|  4400| 36| Marketing|\n",
      "|   201| Michael| 13000| 32|        IT|\n",
      "|   202|     Pat|  6000| 39|        HR|\n",
      "+------+--------+------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.createTempView('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---+----------+\n",
      "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
      "+------+-----------+------+---+----------+\n",
      "|   199|    Douglas|  2600| 34|     Sales|\n",
      "|   200|   Jennifer|  4400| 36| Marketing|\n",
      "|   201|    Michael| 13000| 32|        IT|\n",
      "|   202|        Pat|  6000| 39|        HR|\n",
      "|   203|      Susan|  6500| 36| Marketing|\n",
      "|   205|    Shelley| 12008| 33|   Finance|\n",
      "|   206|    William|  8300| 37|        IT|\n",
      "|   100|     Steven| 24000| 39|        IT|\n",
      "|   102|        Lex| 17000| 37| Marketing|\n",
      "|   103|  Alexander|  9000| 39| Marketing|\n",
      "|   104|      Bruce|  6000| 38|        IT|\n",
      "|   105|      David|  4800| 39|        IT|\n",
      "|   106|      Valli|  4800| 38|     Sales|\n",
      "|   107|      Diana|  4200| 35|     Sales|\n",
      "|   109|     Daniel|  9000| 35|        HR|\n",
      "|   110|       John|  8200| 31| Marketing|\n",
      "|   111|     Ismael|  7700| 32|        IT|\n",
      "|   112|Jose Manuel|  7800| 34|        HR|\n",
      "|   113|       Luis|  6900| 34|     Sales|\n",
      "|   116|     Shelli|  2900| 37|   Finance|\n",
      "+------+-----------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('select * from employees where Age > 30')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|Department|   average_salary|\n",
      "+----------+-----------------+\n",
      "|     Sales|5492.923076923077|\n",
      "|        HR|           5837.5|\n",
      "|   Finance|           5730.8|\n",
      "| Marketing|6633.333333333333|\n",
      "|        IT|           7400.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"select Department, AVG(Salary) as average_salary from employees group by Department\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+----------+\n",
      "|Emp_No|Emp_Name|Salary|Age|Department|\n",
      "+------+--------+------+---+----------+\n",
      "|   198|  Donald|  2600| 29|        IT|\n",
      "|   201| Michael| 13000| 32|        IT|\n",
      "|   206| William|  8300| 37|        IT|\n",
      "|   100|  Steven| 24000| 39|        IT|\n",
      "|   104|   Bruce|  6000| 38|        IT|\n",
      "|   105|   David|  4800| 39|        IT|\n",
      "|   111|  Ismael|  7700| 32|        IT|\n",
      "|   129|   Laura|  3300| 38|        IT|\n",
      "|   132|      TJ|  2100| 34|        IT|\n",
      "|   136|   Hazel|  2200| 29|        IT|\n",
      "+------+--------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"select * from employees where Department = 'IT'\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+----------+----------------+\n",
      "|Emp_No|Emp_Name|Salary|Age|Department|SalaryAfterBonus|\n",
      "+------+--------+------+---+----------+----------------+\n",
      "|   198|  Donald|  2600| 29|        IT|          2860.0|\n",
      "|   199| Douglas|  2600| 34|     Sales|          2860.0|\n",
      "|   200|Jennifer|  4400| 36| Marketing|          4840.0|\n",
      "|   201| Michael| 13000| 32|        IT|         14300.0|\n",
      "|   202|     Pat|  6000| 39|        HR|          6600.0|\n",
      "+------+--------+------+---+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "employee_df = employee_df.withColumn(\"SalaryAfterBonus\", expr(\"Salary * 1.1\"))\n",
    "employee_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|Age|maximum_salary|\n",
      "+---+--------------+\n",
      "| 31|          8200|\n",
      "| 34|          7800|\n",
      "| 28|         12008|\n",
      "| 27|         17000|\n",
      "| 26|          3600|\n",
      "+---+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"select Age, Max(Salary) as maximum_salary from employees group by Age\")\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+----------+----------------+--------+------+---+----------+----------------+\n",
      "|Emp_No|Emp_Name|Salary|Age|Department|SalaryAfterBonus|Emp_Name|Salary|Age|Department|SalaryAfterBonus|\n",
      "+------+--------+------+---+----------+----------------+--------+------+---+----------+----------------+\n",
      "|   198|  Donald|  2600| 29|        IT|          2860.0|  Donald|  2600| 29|        IT|          2860.0|\n",
      "|   199| Douglas|  2600| 34|     Sales|          2860.0| Douglas|  2600| 34|     Sales|          2860.0|\n",
      "|   200|Jennifer|  4400| 36| Marketing|          4840.0|Jennifer|  4400| 36| Marketing|          4840.0|\n",
      "|   201| Michael| 13000| 32|        IT|         14300.0| Michael| 13000| 32|        IT|         14300.0|\n",
      "|   202|     Pat|  6000| 39|        HR|          6600.0|     Pat|  6000| 39|        HR|          6600.0|\n",
      "+------+--------+------+---+----------+----------------+--------+------+---+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = employee_df.join(employee_df, 'Emp_No', 'inner')\n",
    "join_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(Age)|\n",
      "+--------+\n",
      "|   33.56|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg \n",
    "\n",
    "average_age = employee_df.agg(avg(\"Age\")).alias(\"Average_Age\")\n",
    "average_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+----------------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|SalaryAfterBonus|\n",
      "+------+---------+------+---+----------+----------------+\n",
      "|   137|   Renske|  3600| 26| Marketing|          3960.0|\n",
      "|   101|    Neena| 17000| 27|     Sales|         18700.0|\n",
      "|   114|      Den| 11000| 27|   Finance|         12100.0|\n",
      "|   108|    Nancy| 12008| 28|     Sales|         13208.8|\n",
      "|   130|    Mozhe|  2800| 28| Marketing|          3080.0|\n",
      "|   126|    Irene|  2700| 28|        HR|          2970.0|\n",
      "|   204|  Hermann| 10000| 29|   Finance|         11000.0|\n",
      "|   115|Alexander|  3100| 29|   Finance|          3410.0|\n",
      "|   134|  Michael|  2900| 29|     Sales|          3190.0|\n",
      "|   198|   Donald|  2600| 29|        IT|          2860.0|\n",
      "|   140|   Joshua|  2500| 29|   Finance|          2750.0|\n",
      "|   136|    Hazel|  2200| 29|        IT|          2420.0|\n",
      "|   120|  Matthew|  8000| 30|        HR|          8800.0|\n",
      "|   110|     John|  8200| 31| Marketing|          9020.0|\n",
      "|   127|    James|  2400| 31|        HR|          2640.0|\n",
      "|   201|  Michael| 13000| 32|        IT|         14300.0|\n",
      "|   111|   Ismael|  7700| 32|        IT|          8470.0|\n",
      "|   119|    Karen|  2500| 32|   Finance|          2750.0|\n",
      "|   205|  Shelley| 12008| 33|   Finance|         13208.8|\n",
      "|   124|    Kevin|  5800| 33| Marketing|          6380.0|\n",
      "+------+---------+------+---+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc \n",
    "\n",
    "employee_df = employee_df.orderBy(asc(\"Age\"), desc(\"Salary\"))\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|Department|count(Emp_No)|\n",
      "+----------+-------------+\n",
      "|     Sales|           13|\n",
      "|        HR|            8|\n",
      "|   Finance|           10|\n",
      "| Marketing|            9|\n",
      "|        IT|           10|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count \n",
    "\n",
    "employ_count = employee_df.groupBy(\"Department\").agg(count(\"Emp_No\")).alias('Number_Employee')\n",
    "employ_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---+----------+\n",
      "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
      "+------+-----------+------+---+----------+\n",
      "|   198|     Donald|  2600| 29|        IT|\n",
      "|   199|    Douglas|  2600| 34|     Sales|\n",
      "|   110|       John|  8200| 31| Marketing|\n",
      "|   112|Jose Manuel|  7800| 34|        HR|\n",
      "|   130|      Mozhe|  2800| 28| Marketing|\n",
      "|   133|      Jason|  3300| 38|     Sales|\n",
      "|   139|       John|  2700| 36|     Sales|\n",
      "|   140|     Joshua|  2500| 29|   Finance|\n",
      "+------+-----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"select * from employees where Emp_Name like '%o%'\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
